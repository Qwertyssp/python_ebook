==tokenize 模块==


``tokenize`` 模块将一段 Python 源文件分割成不同的 token . 
你可以在代码高亮工具中使用它.

在 [Example 13-17 #eg-13-17] 中, 我们分别打印出这些 token .

====Example 13-17. 使用 tokenize 模块====[eg-13-17]

```
File: tokenize-example-1.py

import tokenize

file = open("tokenize-example-1.py")

def handle_token(type, token, (srow, scol), (erow, ecol), line):
    print "%d,%d-%d,%d:\t%s\t%s" % \
        (srow, scol, erow, ecol, tokenize.tok_name[type], repr(token))

tokenize.tokenize(
    file.readline,
    handle_token
    )

*B*1,0-1,6:     NAME    'import'
1,7-1,15:    NAME    'tokenize'
1,15-1,16:   NEWLINE '\012'
2,0-2,1:     NL      '\012'
3,0-3,4:     NAME    'file'
3,5-3,6:     OP      '='
3,7-3,11:    NAME    'open'
3,11-3,12:   OP      '('
3,12-3,35:   STRING  '"tokenize-example-1.py"'
3,35-3,36:   OP      ')'
3,36-3,37:   NEWLINE '\012'
...*b*
```

注意这里的 ``tokenize`` 函数接受两个可调用对象作为参数: 前一个用于获取新的代码行, 
第二个用于在获得每个 token 时调用.